{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "693bf89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from icecream import ic\n",
    "import json\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb0903",
   "metadata": {},
   "source": [
    "## load variables - rock_dams\n",
    "- data folder\n",
    "- cleaned_df - for only rock_dams\n",
    "- json template\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e377bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(),'data')\n",
    "cleaned_csv = os.path.join(data_folder, 'final_range_data.csv')\n",
    "dff = pd.read_csv(cleaned_csv)\n",
    "\n",
    "#lambda function to filter for rock_dams\n",
    "#for each row, is it a list/str, then is 'rock_dams' in it, if so, return the df.\n",
    "cleaned_df = dff[\n",
    "    dff[\"land_use_selection-intervention\"]\n",
    "    .apply(lambda x: isinstance(x, (list, str)) and \"rock_dam\" in x)\n",
    "]\n",
    "\n",
    "#rock_dam datasets, not there, so not needed\n",
    "\n",
    "TEMPLATE_FILE = os.path.join(data_folder,\"halfmoon_json_template.json\")\n",
    "with open(TEMPLATE_FILE, \"r\") as f:\n",
    "    template = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc8e906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mi\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mhalfmoons contour_planting water_points rock_dam\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mcount\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mrock_dam count\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mcleaned_df\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('rock_dam count', 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dff[\"land_use_selection-intervention\"]\n",
    "count = 0\n",
    "for idx, i in enumerate(list(x)):\n",
    "    if 'rock_dam' in i:\n",
    "        count += 1\n",
    "        ic(i, count)\n",
    "\n",
    "ic('rock_dam count',len(cleaned_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb72fc",
   "metadata": {},
   "source": [
    "## enumerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c59a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(list(dff['basic_info-name'].unique()))\n",
    "##empty name and key dictionary\n",
    "names_dict = {}\n",
    "for idx, row in dff.iterrows():\n",
    "    #ic({row[\"KEY\"], row[\"basic_info-name\"]})\n",
    "    names_dict[row[\"KEY\"]] = row[\"basic_info-name\"]\n",
    "#ic(names_dict)\n",
    "name_uname_dict = {\n",
    "    \"Zakariye haji ali\": \"abdullahi.hassan@mardo.org\", \n",
    "    \"Issack Idow Ibrahim\": \"abdullahi.hassan@mardo.org\",\n",
    "    \"Abdullahi Hassan Adan\": \"abdullahi.hassan@mardo.org\",\n",
    "    \"Abdikheir Mohamed\": \"mukhtar.yusuf23\",\n",
    "    \"Abdikheir Mohamed Ali\": \"mukhtar.yusuf23\",\n",
    "    \"Abdikheir Mohamed Dahir\": \"mukhtar.yusuf23\",\n",
    "    \"Ali\": \"mukhtar.yusuf23\",\n",
    "    \"Ali Adan\": \"mukhtar.yusuf23\",\n",
    "    \"Ali Aden\": \"mukhtar.yusuf23\",\n",
    "    \"Fathi\": \"mukhtar.yusuf23\", \n",
    "    \"Fathi Abdirashid\": \"mukhtar.yusuf23\",\n",
    "    \"Ahmed Omar Hassan\": \"AhmedOmar\",\n",
    "    \"Maria Abdirahman Ali\": \"AhmedOmar\",\n",
    "    \"ABSHIR ABDULLAH ALI\": \"Ahil\",\n",
    "    \"Ahmed Ibrahim\": \"JubaFoundation\",\n",
    "    \"Ahmed Ibrahim Mohamed\": \"JubaFoundation\",\n",
    "    \"Abdiaziz Adan Hassa\": \"guudow\",\n",
    "    \"Abdiaziz Adan Hassan\": \"guudow\",\n",
    "    \"Abdi Hassan Adan\": \"AbdiHassan\",\n",
    "    \"Abdullahi\": \"sharif1234\",\n",
    "    \"Abdirashid Sheikh Mohamed\": \"NRMO\",\n",
    "    \"Abdullahi Sharif\": \"sharif1234\",\n",
    "    \"Abdullahi Sharif LLG\": \"sharif1234\",\n",
    "    \"Abdullahi Sharif Noor LLG\": \"sharif1234\",\n",
    "    \"Abdijalil\": \"Csxaashi\",\n",
    "    \"Abdijalil said\": \"Csxaashi\",\n",
    "    \"Abdikarim Mohamed Ismail\": \"samandoulgou\",\n",
    "    \"Abdikarin Mohamed Ismail\": \"samandoulgou\",\n",
    "    \"Hassan Omar Khadhib\": \"Khadhib\",\n",
    "    \"Hassan Omar Khadhibr\": \"Khadhib\",\n",
    "    \"Jamal Ali\": \"JamalAli\",\n",
    "    \"Jamal Ali Tagal\": \"JamalAli\",\n",
    "    \"Mohamud Abdullahi Ali\": \"Mohamud\",\n",
    "    \"Moulid Abdullahi Abdi\": \"Moulidwadani\",\n",
    "    \"Muktar Mohamednoor Rage\": \"Mrage1997\",\n",
    "    \"Osman Ebey Omar\": \"iibey\",\n",
    "    \"Yusuf Mahi munye\": \"Mcnbdn617660\",\n",
    "    \"abdullahi Sharif LLG\": \"sharif1234\",\n",
    "    \"osman Ebey Omar\": \"iibey\",\n",
    "}\n",
    "\n",
    "## match and replace names with usernames\n",
    "for k, v in names_dict.items():\n",
    "    for key, value in name_uname_dict.items():\n",
    "        if v == key:\n",
    "            names_dict[k] = value\n",
    "#ic(names_dict)\n",
    "\n",
    "##enumerator dictionary\n",
    "enumerator = {\n",
    "        \"id\": 3726,\n",
    "        \"first_name\": \"First\",\n",
    "        \"last_name\": \"Last\",\n",
    "        \"gender\": \"Male\",\n",
    "        \"age_category\": \"18 - 35\",\n",
    "        \"phone_number\": \"0e15704046\",\n",
    "        \"email\": \"test_mail@gmail.com\",\n",
    "        \"country\": \"test\",\n",
    "        \"organization\": \"Test\",\n",
    "        \"username\": \"test\",\n",
    "        \"type\": \"ENUMERATOR\"\n",
    "    }\n",
    "\n",
    "enums = []\n",
    "#create enumerator jsons\n",
    "for key, value in names_dict.items():\n",
    "    e = deepcopy(enumerator)\n",
    "    e['username'] = value\n",
    "    e['key'] = key\n",
    "    enums.append(e)\n",
    "\n",
    "#ic(enums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5003e3",
   "metadata": {},
   "source": [
    "## farming entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9108414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "farming_entity_list = []\n",
    "for idx, i in enumerate(cleaned_df.to_dict(orient='records'), start=1):\n",
    "    farming_entity = {}\n",
    "    #ic(i['basic_info-organisation_name'])\n",
    "    farming_entity['id'] = idx\n",
    "    farming_entity['first_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['middle_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['last_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['organization'] = 'INSTITUTION'\n",
    "    farming_entity['key'] = i['KEY']\n",
    "    \n",
    "    farming_entity_list.append(\n",
    "        {\n",
    "            \"farmingEntity\": farming_entity, \n",
    "            'key': i['KEY']\n",
    "        }\n",
    "    )\n",
    "    #ic(farming_entity_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded92b8",
   "metadata": {},
   "source": [
    "# plot-rock_dams details\n",
    "- read df to dictionary, for matching keys, extract df values\n",
    "### plot-points\n",
    "- parse the geoshape column from cleaned df\n",
    "- split and extract the values from each i\n",
    "- append to json keys\n",
    "\n",
    "## crops\n",
    "not present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe36f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import copy\n",
    "import re\n",
    "\n",
    "plot_list = []\n",
    "point_id = 1\n",
    "\n",
    "records = cleaned_df.to_dict(orient='records')\n",
    "\n",
    "count = 0\n",
    "#iterate over every i to capture plot details\n",
    "for idx, i in enumerate(records):\n",
    "    plot_id = idx + 1\n",
    "    count += 1\n",
    "\n",
    "    # ---- BUILD PLOT DETAILS ----\n",
    "    template_cp = copy.deepcopy(template[0])\n",
    "    plot_details = template_cp['plot']\n",
    "\n",
    "    plot_details['id'] = plot_id\n",
    "    plot_details['key'] = i['KEY']\n",
    "    plot_details['name'] = uuid.uuid4().hex\n",
    "    plot_details['calculated_size'] = i['area_ha']\n",
    "    plot_details['photo_url'] = i['plot_details-photo']\n",
    "    plot_details['farmingEntityId'] = farming_entity['id']\n",
    "    plot_details['livestock_allowed'] = False if i['land_use_selection-land_use'] == 'agriculture' else True\n",
    "    plot_details['conservation_area'] = False\n",
    "\n",
    "\n",
    "    #holds every row's crops\n",
    "    all_crops = []\n",
    "    \"\"\"----------------------------------CROP STATUS ----------------------------------\"\"\"\n",
    "    #not in child table, so skipped\n",
    "        \n",
    "    \"\"\"--------------------------------GEOSHAPE PARSING ----------------------------------\"\"\"\n",
    "    geoshape_str = i['plot_details-polygon']\n",
    "    \n",
    "    def parse_geoshape(geoshape_str):\n",
    "        #parse a geoshape string into a list of point dictionaries\n",
    "        points = []\n",
    "        parts = geoshape_str.split(';')\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            lat, lon, alt, acc = map(float, part.split(' '))\n",
    "\n",
    "            points.append({\n",
    "                \"longitude\": lon,\n",
    "                \"latitude\": lat,\n",
    "                \"altitude\": alt,\n",
    "                \"accuracy\": acc\n",
    "            })\n",
    "\n",
    "        return points\n",
    "\n",
    "    parsed_points = parse_geoshape(geoshape_str)\n",
    "\n",
    "    points_list = []\n",
    "    for p in parsed_points:\n",
    "        points_list.append({\n",
    "            \"id\": point_id,\n",
    "            \"plotId\": plot_id,\n",
    "            \"longitude\": p[\"longitude\"],\n",
    "            \"latitude\": p[\"latitude\"],\n",
    "            \"altitude\": p[\"altitude\"],\n",
    "            \"accuracy\": p[\"accuracy\"]\n",
    "        })\n",
    "        point_id += 1\n",
    "\n",
    "\n",
    "    \"\"\"--------------------------------ADMINISTRATIVE DETAILS ----------------------------------\"\"\"\n",
    "    administrative = plot_details['subCounty']\n",
    "    administrative['subcounty_name'] = i['geography-district_name']\n",
    "    administrative['county']['county_name'] = i['geography-region_name']\n",
    "\n",
    "\n",
    "    #update plot_details with number of points and administrative info\n",
    "    plot_details['points'] = points_list\n",
    "    plot_details['subCounty'] = administrative\n",
    "    \n",
    "    # del plot_details['points']\n",
    "    # del plot_details['subCounty']\n",
    "    general_plot = {\n",
    "        \"plot\":plot_details, \n",
    "        'key':i['KEY']\n",
    "    }\n",
    "    #ic(i['KEY'])\n",
    "    plot_list.append(\n",
    "        general_plot\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10a28b",
   "metadata": {},
   "source": [
    "## project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c80b180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mset\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mnames\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m{\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mBRCiS III\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m}\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BRCiS III'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects_list = []\n",
    "\n",
    "names =[]\n",
    "for row in cleaned_df.to_dict(orient='records'):\n",
    "    name = row['land_use_selection-project_name']\n",
    "    \n",
    "    if name == 'terra':\n",
    "        name = name.upper()\n",
    "        id = 131\n",
    "    elif name == 'brcis3':\n",
    "        name = 'BRCiS III'\n",
    "        id = 93\n",
    "    \n",
    "    names.extend([name])\n",
    "    \n",
    "    projects_list.append({\n",
    "        \"project\": {\n",
    "            \"id\": id,\n",
    "            \"project_name\": name\n",
    "        },\n",
    "        \"key\": row['KEY']\n",
    "    })\n",
    "\n",
    "ic(set(names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466d219",
   "metadata": {},
   "source": [
    "## Rock Dams\n",
    "- no info whatsoever in child table.\n",
    "- only take from parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "298a1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_dams_list = []\n",
    "\n",
    "#dictionary to map odk values to RA app schema values\n",
    "mapping_dict = {\n",
    "    \"STICKS_BRANCH\":\"STICKS_PROTECTION\",\n",
    "    \"DEBRIS_REMOVAL\":\"DEBRIS_REMOVAL\",\n",
    "    \"INVASIVE_SPECIES\":\"INVASIVE_CONTROL\",\n",
    "    \"WEEDING\":\"WEEDING\",\n",
    "    \"WATERING\":\"WATERING\",\n",
    "    \"PLANTING_SEEDLING\":\"PLANTING_ANNUAL_PERENNIAL_CROPS\"\n",
    "}\n",
    "\n",
    "for idx, c in enumerate(records):\n",
    "\n",
    "    \"\"\"-----------------------------------BUNDS CHILD DETAILS-------------------------------------\"\"\"\n",
    "    #not present\n",
    "    \"\"\"-----------------------------------Terracing DETAILS-------------------------------------\"\"\"\n",
    "\n",
    "    rock_dams_details = {\n",
    "        \"id\": idx + 1,\n",
    "        \"key\":c[\"KEY\"],\n",
    "        \"erosion_control_type\": \"rock_dams\",\n",
    "        \"established_date\": datetime.strptime(str(c['rock_dams-date']), \"%d/%m/%Y\").strftime(\"%Y-%m\"),\n",
    "        \"material_used\": [\n",
    "            \"Rocks\"\n",
    "        ],\n",
    "        \"other_material_used\": \"\",\n",
    "        \"who_established\": [\"MALE\", \"FEMALE\"] if c[\"rock_dams-manages\"] == 'both' else \"MALE\" if c[\"rock_dams-manages\"]==\"male\" else \"\",\n",
    "        \"other_who_established\": \"YOUTH\" if c[\"rock_dams-youth_manages\"] == 'yes' else \"\",\n",
    "        \"total_interventions\": c[\"rock_dams-total\"],\n",
    "        \"length\": c[\"rock_dams-length\"],\n",
    "        \"width\": c[\"rock_dams-width\"],\n",
    "        \"depth\": c[\"rock_dams-depth\"],\n",
    "        \"vertical_spacing\": 0,\n",
    "        \"horizontal_spacing\": 0,\n",
    "        \"labor_payment\": c[\"rock_dams-paid_unpaid_labor\"],        \n",
    "        \"has_grass\": False,\n",
    "        \"total_number_grasses_planted\": 0,\n",
    "        \"kg_grass_seeds_planted\": 0,\n",
    "        \"has_trees\": False,\n",
    "        \"total_number_different_trees_planted\": 0,\n",
    "        \"total_number_trees_planted\": 0,\n",
    "        \"total_number_trees_survived\": 0,\n",
    "        \"management_practices\":  [],\n",
    "        \"other_managements\": \" \",\n",
    "        \"usages\":[\"MIGRATED_NOT_KNOWN\"],\n",
    "        \"other_usages\": \"\",\n",
    "        \"rangelandEntryId\": 2,\n",
    "        \"currentStatus\": [],\n",
    "        \"grassEstablishment\": [],\n",
    "        \"treeEstablishment\":[]  \n",
    "    }\n",
    "\n",
    "    rock_dams_list.append(rock_dams_details)\n",
    "    \n",
    "\n",
    "\n",
    "rdams_econtrol = [{\"econtrol\": [i], \"key\" :i[\"key\"]} for i in rock_dams_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df9e208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(rdams_econtrol, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8a741",
   "metadata": {},
   "source": [
    "## merge components into one json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51ac36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mfinal\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component lists\n",
    "lists = [farming_entity_list, rdams_econtrol,plot_list, projects_list]\n",
    "\n",
    "# The output dictionary\n",
    "merged = {}\n",
    "\n",
    "for lst in lists:\n",
    "    ic(len(lst))\n",
    "    for item in lst:\n",
    "        k = item['key']\n",
    "\n",
    "        #check if already updated\n",
    "        if k not in merged:\n",
    "            merged[k] = {}\n",
    "            \n",
    "        #merge the item into the existing dictionary\n",
    "        merged[k].update(item)\n",
    "\n",
    "final = list(merged.values())\n",
    "for i in final:\n",
    "    #match enumerators\n",
    "    for l in enums:\n",
    "        if i['key'] == l['key']:\n",
    "            i['enumerator'] = l\n",
    "\n",
    "\n",
    "    i[\"boma\"] = []\n",
    "    i[\"cgrazing\"] = []\n",
    "    i[\"microcatchment\"]= []\n",
    "    i[\"seedbank\"] = []\n",
    "    i[\"seeding\"] = []\n",
    "    i[\"iremoval\"] = []\n",
    "    i[\"waterpoint\"] = []\n",
    "    i[\"completed\"] = True\n",
    "    i[\"rangeModuleId\"] = 1\n",
    "    i[\"currentStep\"]= \"DONE\"\n",
    "    i[\"moduleId\"] = 6\n",
    "    i[\"is_revisit\"] = False\n",
    "    #get the date_collected\n",
    "    for j in records:\n",
    "        if i[\"key\"] == j[\"KEY\"]:\n",
    "            i[\"date_collected\"] = datetime.strptime(j['basic_info-date'], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "    #match project id\n",
    "    for x in projects_list:\n",
    "        if i['key'] == x['key']:\n",
    "            i['projectId'] = x['project']['id']\n",
    "    #match plot id\n",
    "    for k in plot_list:\n",
    "        if i['key'] == k['key']:\n",
    "            i['plotId'] = k['plot']['id']\n",
    "    #match farming_entity_id\n",
    "    for h in farming_entity_list:\n",
    "        if i['key'] == h['key']:\n",
    "            i['farmingEntityId'] = h['farmingEntity']['id']\n",
    "    #match enumerator id\n",
    "    for e in enums:\n",
    "        if i['key'] == e['key']:\n",
    "            i['enumeratorId'] = e['id']\n",
    "\n",
    "\n",
    "ic(len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4af25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(final, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4593fab",
   "metadata": {},
   "source": [
    "## clean up keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fbd8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up keys\n",
    "def remove_keys(obj, key=\"key\"):\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(key, None)\n",
    "        for v in obj.values():\n",
    "            remove_keys(v, key)\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            remove_keys(v, key)\n",
    "    return obj\n",
    "\n",
    "finaal = [x for x in final if x['key'] == 'uuid:b4a2d545-66fe-4d23-825b-41025149b460']\n",
    "finaal = [remove_keys(i) for i in final]\n",
    "\n",
    "#add id\n",
    "for idx, c in enumerate(finaal):\n",
    "    c['id'] = idx + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "407368d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36m✅\u001b[39m\u001b[38;5;36m'\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(finaal):\n",
    "    if len(i[1].keys()) != 23:\n",
    "        ic(\"ERROR\")\n",
    "    else:\n",
    "        ic(\"✅\")\n",
    "        with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "            json.dump(finaal, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
