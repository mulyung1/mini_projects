{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66b1df2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from icecream import ic\n",
    "import json\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a257f7",
   "metadata": {},
   "source": [
    "## load variables - invasive species\n",
    "- data folder\n",
    "- cleaned_df - for only invasive species\n",
    "- json template\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aedd7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(),'data')\n",
    "cleaned_csv = os.path.join(data_folder, 'final_range_data.csv')\n",
    "dff = pd.read_csv(cleaned_csv)\n",
    "\n",
    "#lambda function to filter for invasives\n",
    "#for each row, is it a list/str, then is 'invasives' in it, if so, return the df.\n",
    "cleaned_df = dff[\n",
    "    dff[\"land_use_selection-intervention\"]\n",
    "    .apply(lambda x: isinstance(x, (list, str)) and \" invasive_removal\" in x)\n",
    "]\n",
    "\n",
    "#no ivasive species datasets\n",
    "\n",
    "TEMPLATE_FILE = os.path.join(data_folder,\"halfmoon_json_template.json\")\n",
    "with open(TEMPLATE_FILE, \"r\") as f:\n",
    "    template = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782fbaa",
   "metadata": {},
   "source": [
    "## check invasives count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebaaca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mi\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mhalfmoons water_points invasive_removal\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mcount\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36minvasive_removal count\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mcleaned_df\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('invasive_removal count', 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dff[\"land_use_selection-intervention\"]\n",
    "count = 0\n",
    "for idx, i in enumerate(list(x)):\n",
    "    if 'invasive_removal' in i:\n",
    "        count += 1\n",
    "        ic(i, count)\n",
    "\n",
    "ic('invasive_removal count',len(cleaned_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08155b96",
   "metadata": {},
   "source": [
    "## enumerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01ef5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(list(dff['basic_info-name'].unique()))\n",
    "##empty name and key dictionary\n",
    "names_dict = {}\n",
    "for idx, row in dff.iterrows():\n",
    "    #ic({row[\"KEY\"], row[\"basic_info-name\"]})\n",
    "    names_dict[row[\"KEY\"]] = row[\"basic_info-name\"]\n",
    "#ic(names_dict)\n",
    "name_uname_dict = {\n",
    "    \"Zakariye haji ali\": \"abdullahi.hassan@mardo.org\", \n",
    "    \"Issack Idow Ibrahim\": \"abdullahi.hassan@mardo.org\",\n",
    "    \"Abdullahi Hassan Adan\": \"abdullahi.hassan@mardo.org\",\n",
    "    \"Abdikheir Mohamed\": \"mukhtar.yusuf23\",\n",
    "    \"Abdikheir Mohamed Ali\": \"mukhtar.yusuf23\",\n",
    "    \"Abdikheir Mohamed Dahir\": \"mukhtar.yusuf23\",\n",
    "    \"Ali\": \"mukhtar.yusuf23\",\n",
    "    \"Ali Adan\": \"mukhtar.yusuf23\",\n",
    "    \"Ali Aden\": \"mukhtar.yusuf23\",\n",
    "    \"Fathi\": \"mukhtar.yusuf23\", \n",
    "    \"Fathi Abdirashid\": \"mukhtar.yusuf23\",\n",
    "    \"Ahmed Omar Hassan\": \"AhmedOmar\",\n",
    "    \"Maria Abdirahman Ali\": \"AhmedOmar\",\n",
    "    \"ABSHIR ABDULLAH ALI\": \"Ahil\",\n",
    "    \"Ahmed Ibrahim\": \"JubaFoundation\",\n",
    "    \"Ahmed Ibrahim Mohamed\": \"JubaFoundation\",\n",
    "    \"Abdiaziz Adan Hassa\": \"guudow\",\n",
    "    \"Abdiaziz Adan Hassan\": \"guudow\",\n",
    "    \"Abdi Hassan Adan\": \"AbdiHassan\",\n",
    "    \"Abdullahi\": \"sharif1234\",\n",
    "    \"Abdirashid Sheikh Mohamed\": \"NRMO\",\n",
    "    \"Abdullahi Sharif\": \"sharif1234\",\n",
    "    \"Abdullahi Sharif LLG\": \"sharif1234\",\n",
    "    \"Abdullahi Sharif Noor LLG\": \"sharif1234\",\n",
    "    \"Abdijalil\": \"Csxaashi\",\n",
    "    \"Abdijalil said\": \"Csxaashi\",\n",
    "    \"Abdikarim Mohamed Ismail\": \"samandoulgou\",\n",
    "    \"Abdikarin Mohamed Ismail\": \"samandoulgou\",\n",
    "    \"Hassan Omar Khadhib\": \"Khadhib\",\n",
    "    \"Hassan Omar Khadhibr\": \"Khadhib\",\n",
    "    \"Jamal Ali\": \"JamalAli\",\n",
    "    \"Jamal Ali Tagal\": \"JamalAli\",\n",
    "    \"Mohamud Abdullahi Ali\": \"Mohamud\",\n",
    "    \"Moulid Abdullahi Abdi\": \"Moulidwadani\",\n",
    "    \"Muktar Mohamednoor Rage\": \"Mrage1997\",\n",
    "    \"Osman Ebey Omar\": \"iibey\",\n",
    "    \"Yusuf Mahi munye\": \"Mcnbdn617660\",\n",
    "    \"abdullahi Sharif LLG\": \"sharif1234\",\n",
    "    \"osman Ebey Omar\": \"iibey\",\n",
    "}\n",
    "\n",
    "## match and replace names with usernames\n",
    "for k, v in names_dict.items():\n",
    "    for key, value in name_uname_dict.items():\n",
    "        if v == key:\n",
    "            names_dict[k] = value\n",
    "#ic(names_dict)\n",
    "\n",
    "##enumerator dictionary\n",
    "enumerator = {\n",
    "        \"id\": 3726,\n",
    "        \"first_name\": \"First\",\n",
    "        \"last_name\": \"Last\",\n",
    "        \"gender\": \"Male\",\n",
    "        \"age_category\": \"18 - 35\",\n",
    "        \"phone_number\": \"0e15704046\",\n",
    "        \"email\": \"test_mail@gmail.com\",\n",
    "        \"country\": \"test\",\n",
    "        \"organization\": \"Test\",\n",
    "        \"username\": \"test\",\n",
    "        \"type\": \"ENUMERATOR\"\n",
    "    }\n",
    "\n",
    "enums = []\n",
    "#create enumerator jsons\n",
    "for key, value in names_dict.items():\n",
    "    e = deepcopy(enumerator)\n",
    "    e['username'] = value\n",
    "    e['key'] = key\n",
    "    enums.append(e)\n",
    "\n",
    "#ic(enums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977ec96",
   "metadata": {},
   "source": [
    "## farming entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1f40174",
   "metadata": {},
   "outputs": [],
   "source": [
    "farming_entity_list = []\n",
    "for idx, i in enumerate(cleaned_df.to_dict(orient='records'), start=1):\n",
    "    farming_entity = {}\n",
    "    #ic(i['basic_info-organisation_name'])\n",
    "    farming_entity['id'] = idx\n",
    "    farming_entity['first_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['middle_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['last_name'] = i['basic_info-organisation_name']\n",
    "    farming_entity['organization'] = 'INSTITUTION'\n",
    "    farming_entity['key'] = i['KEY']\n",
    "    \n",
    "    farming_entity_list.append(\n",
    "        {\n",
    "            \"farmingEntity\": farming_entity, \n",
    "            'key': i['KEY']\n",
    "        }\n",
    "    )\n",
    "    #ic(farming_entity_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe251fe5",
   "metadata": {},
   "source": [
    "## plot-invasives_details\n",
    "- read df to dictionary, for matching keys, extract df values\n",
    "### plot-points\n",
    "- parse the geoshape column from cleaned df\n",
    "- split and extract the values from each i\n",
    "- append to json keys\n",
    "\n",
    "## crops\n",
    "- no need for crop details as repeat is not there\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad1ec0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mgeoshape_str\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36m6.6514281 46.9264623 309.0 7.833;6.6514948 46.926457 316.0 2.38;6.6516772 46.926446 318.0 2.316;6.651778 46.9264377 317.0 2.38;6.651948 46.9264147 319.0 2.28;6.6521508 46.9263863 321.0 2.38;6.6523469 46.9263628 321.0 2.38;6.6524931 46.9263556 320.0 2.316;6.6526859 46.9263277 321.0 2.316;6.6528936 46.9263071 321.0 2.38;6.6530636 46.9262831 322.0 2.316;6.6532716 46.9262469 321.0 2.316;6.6534731 46.9262218 321.0 2.38;6.653641 46.9261973 321.0 2.08;6.6538445 46.9261723 319.0 2.28;6.6540471 46.9261441 319.0 2.38;6.6542244 46.9261217 318.0 2.316;6.6544386 46.9260928 319.0 2.38;6.6546394 46.9260635 319.0 2.316;6.6548222 46.9260321 319.0 2.38;6.6550299 46.926028 318.0 2.38;6.6552391 46.9259801 318.0 2.233;6.6554134 46.925953 317.0 2.38;6.6554412 46.9260398 319.0 2.233;6.6553898 46.9262381 319.0 2.28;6.655322 46.9264254 317.0 2.233;6.6552517 46.9266383 318.0 2.316;6.6551833 46.9268638 318.0 2.38;6.655136 46.9270228 318.0 2.38;6.655067 46.9272242 317.0 2.316;6.6549976 46.9274451 319.0 2.38;6.6549363 46.9276165 319.0 2.316;6.6548893 46.9278145 316.0 2.38;6.6548116 46.9280362 320.0 2.38;6.654754 46.9282121 320.0 2.38;6.6546913 46.9284176 320.0 2.38;6.6546241 46.9286403 318.0 2.316;6.6545522 46.9288123 317.0 2.316;6.6544989 46.9290094 318.0 2.28;6.6544346 46.9292385 319.0 2.38;6.6543793 46.9294198 319.0 2.316;6.6543027 46.9296249 321.0 2.68;6.6542379 46.929833 318.0 2.38;6.6541789 46.9300203 318.0 2.38;6.6541202 46.930237 319.0 2.38;6.6539189 46.9302464 319.0 2.38;6.6537363 46.9302672 317.0 2.733;6.6535117 46.9302754 318.0 2.68;6.6532929 46.9302979 319.0 2.78;6.6530978 46.9303248 319.0 2.78;6.6528721 46.9303271 317.0 2.733;6.652645 46.9303451 318.0 2.78;6.6524576 46.9303626 318.0 2.78;6.6522191 46.9303823 319.0 2.733;6.6520094 46.9304073 317.0 2.78;6.6518235 46.9304262 317.0 2.78;6.6516077 46.9304429 318.0 2.733;6.6513881 46.9304697 318.0 2.65;6.6512308 46.9304925 319.0 2.78;6.6510301 46.9304942 318.0 2.78;6.650963 46.930411 318.0 2.58;6.6509798 46.9302173 316.0 2.733;6.6510106 46.9300074 317.0 2.65;6.651034 46.9297914 316.0 2.58;6.6510501 46.9295958 317.0 2.65;6.6510631 46.9293731 317.0 2.78;6.6510889 46.9291515 316.0 2.68;6.6511172 46.9289602 317.0 2.68;6.6511594 46.9287553 317.0 2.68;6.6511601 46.9285451 317.0 2.68;6.6511741 46.9283637 316.0 2.566;6.6512048 46.9281431 316.0 2.78;6.6512314 46.9279393 315.0 2.566;6.65125 46.9277531 315.0 2.48;6.6512776 46.9275354 315.0 2.38;6.6513084 46.9273273 316.0 2.38;6.6513229 46.9271505 316.0 2.316;6.6513635 46.926924 316.0 2.65;6.6513941 46.9267427 315.0 2.65;6.6513974 46.9265612 317.0 2.78;6.6514098 46.9264782 316.0 2.38;6.6514281 46.9264623 309.0 7.833\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;245m    \u001b[39m\u001b[38;5;247mi\u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mKEY\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36muuid:48f049ba-b292-4c09-a914-7667d1158a10\u001b[39m\u001b[38;5;36m'\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import copy\n",
    "import re\n",
    "\n",
    "def parse_geoshape(geoshape_str):\n",
    "    #parse a geoshape string into a list of point dictionaries\n",
    "    points = []\n",
    "    parts = geoshape_str.split(';')\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        lat, lon, alt, acc = map(float, part.split(' '))\n",
    "\n",
    "        points.append({\n",
    "            \"longitude\": lon,\n",
    "            \"latitude\": lat,\n",
    "            \"altitude\": alt,\n",
    "            \"accuracy\": acc\n",
    "        })\n",
    "\n",
    "    return points\n",
    "\n",
    "plot_list = []\n",
    "point_id = 1\n",
    "\n",
    "records = cleaned_df.to_dict(orient='records')\n",
    "\n",
    "count = 0\n",
    "#iterate over every i to capture plot details\n",
    "for idx, i in enumerate(records):\n",
    "    plot_id = idx + 1\n",
    "    count += 1\n",
    "\n",
    "    # ---- BUILD PLOT DETAILS ----\n",
    "    template_cp = copy.deepcopy(template[0])\n",
    "    plot_details = template_cp['plot']\n",
    "\n",
    "    plot_details['id'] = plot_id\n",
    "    plot_details['key'] = i['KEY']\n",
    "    plot_details['name'] = \"uuid:\" + str(uuid.uuid4())\n",
    "    plot_details['calculated_size'] = i['area_ha']\n",
    "    plot_details['photo_url'] = i['plot_details-photo']\n",
    "    plot_details['farmingEntityId'] = farming_entity['id']\n",
    "    plot_details['livestock_allowed'] = False if i['land_use_selection-land_use'] == 'agriculture' else True\n",
    "    plot_details['conservation_area'] = False\n",
    "    \n",
    "    #no need for crop details \n",
    "        \n",
    "    \"\"\"--------------------------------GEOSHAPE PARSING ----------------------------------\"\"\"\n",
    "    geoshape_str = i['plot_details-polygon']\n",
    "    ic(geoshape_str, i[\"KEY\"])\n",
    "\n",
    "    parsed_points = parse_geoshape(geoshape_str)\n",
    "\n",
    "    points_list = []\n",
    "    for p in parsed_points:\n",
    "        points_list.append({\n",
    "            \"id\": point_id,\n",
    "            \"plotId\": plot_id,\n",
    "            \"longitude\": p[\"longitude\"],\n",
    "            \"latitude\": p[\"latitude\"],\n",
    "            \"altitude\": p[\"altitude\"],\n",
    "            \"accuracy\": p[\"accuracy\"]\n",
    "        })\n",
    "        point_id += 1\n",
    "\n",
    "\n",
    "    \"\"\"--------------------------------ADMINISTRATIVE DETAILS ----------------------------------\"\"\"\n",
    "    administrative = plot_details['subCounty']\n",
    "    administrative['subcounty_name'] = i['geography-district_name']\n",
    "    administrative['county']['county_name'] = i['geography-region_name']\n",
    "\n",
    "\n",
    "    #update plot_details with number of points and administrative info\n",
    "    plot_details['points'] = points_list\n",
    "    plot_details['subCounty'] = administrative\n",
    "    \n",
    "    # del plot_details['points']\n",
    "    # del plot_details['subCounty']\n",
    "    general_plot = {\n",
    "        \"plot\":plot_details, \n",
    "        'key':i['KEY']\n",
    "    }\n",
    "    #ic(i['KEY'])\n",
    "    plot_list.append(\n",
    "        general_plot\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d63f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(plot_list, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bd8c0",
   "metadata": {},
   "source": [
    "## project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e690b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mset\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mnames\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m{\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mBRCiS III\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m}\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BRCiS III'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects_list = []\n",
    "\n",
    "names =[]\n",
    "for row in cleaned_df.to_dict(orient='records'):\n",
    "    name = row['land_use_selection-project_name']\n",
    "    \n",
    "\n",
    "    if name == 'terra':\n",
    "        name = name.upper()\n",
    "        id = 131\n",
    "    elif name == 'brcis3':\n",
    "        name = 'BRCiS III'\n",
    "        id = 93\n",
    "    names.extend([name])\n",
    "    projects_list.append({\n",
    "        \"project\": {\n",
    "            \"id\": id,\n",
    "            \"project_name\": name\n",
    "        },\n",
    "        \"key\": row['KEY']\n",
    "    })\n",
    "\n",
    "ic(set(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b1338",
   "metadata": {},
   "source": [
    "## Invasives \n",
    "missing \n",
    "- herbacious cover\n",
    "- who manages >> used who established\n",
    "- erosion type\n",
    "- is_intervention_effective\n",
    "- vertical_spacing\n",
    "- horizontal_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82787ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247minvasivs_list\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;245m{\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mcomments\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mInvasive_species NONE, iremoval_severity LESS_25\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mgrassCurrentStatus\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mid\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m0\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mkey\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36muuid:48f049ba-b292-4c09-a914-7667d1158a10\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mother_removal_methods\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mphoto_url\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mJPEG_5cc8d00f-cbd5-41a2-a868-374783263033_3612000906371608906.jpg\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mrangelandEntryId\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m8\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mremoval_date\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36m2025-05\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mremoval_methods\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mmanual\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m,\u001b[39m\n",
      "\u001b[38;5;245m                     \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mtreeCurrentStatus\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m}\u001b[39m\u001b[38;5;245m]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "invasivs_list = []\n",
    "\n",
    "\n",
    "for idx, c in enumerate(records):\n",
    "\n",
    "    #if c['KEY'] == \"uuid:1975a725-83a5-4bdd-8305-35ac65b2cce9\":\n",
    "    \"\"\"-----------------------------------INVASIVES CHILD DETAILS-------------------------------------\"\"\"\n",
    "    #no invasve child detail\n",
    "\n",
    "    \"\"\"-----------------------------------INVASIVES DETAILS-------------------------------------\"\"\"\n",
    "\n",
    "    invasives_details = {\n",
    "        \"key\":c[\"KEY\"],\n",
    "        \"id\": idx,\n",
    "        \"removal_date\": datetime.strptime(c['invasive_removal-removal_date'], \"%d/%m/%Y\").strftime(\"%Y-%m\"),\n",
    "        \"removal_methods\": c[\"invasive_removal-removal_methods\"].split(),\n",
    "        \"other_removal_methods\": \"\",\n",
    "        \"photo_url\": \"\", #no photo column\n",
    "        \"comments\": f\"Invasive_species NONE, iremoval_severity {c[\"invasive_removal-severity\"].upper()}\",\n",
    "        \"rangelandEntryId\": 8,\n",
    "        \"grassCurrentStatus\": [],\n",
    "        \"treeCurrentStatus\": []\n",
    "    }\n",
    "\n",
    "    invasivs_list.append(invasives_details)\n",
    "    \n",
    "\n",
    "ic(invasivs_list)\n",
    "iremoval_list = [{\"iremoval\": [i], \"key\" :i[\"key\"]} for i in invasivs_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eface54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(iremoval_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb45db",
   "metadata": {},
   "source": [
    "## merge components into one json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "303b892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mlst\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;32mlen\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;247mfinal\u001b[39m\u001b[38;5;245m)\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m1\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component lists\n",
    "lists = [farming_entity_list, iremoval_list,plot_list, projects_list]\n",
    "\n",
    "# The output dictionary\n",
    "merged = {}\n",
    "\n",
    "for lst in lists:\n",
    "    ic(len(lst))\n",
    "    for item in lst:\n",
    "        k = item['key']\n",
    "\n",
    "        #check if already updated\n",
    "        if k not in merged:\n",
    "            merged[k] = {}\n",
    "            \n",
    "        #merge the item into the existing dictionary\n",
    "        merged[k].update(item)\n",
    "\n",
    "final = list(merged.values())\n",
    "for i in final:\n",
    "    #match enumerators\n",
    "    for l in enums:\n",
    "        if i['key'] == l['key']:\n",
    "            i['enumerator'] = l\n",
    "\n",
    "\n",
    "    i[\"boma\"] = []\n",
    "    i[\"cgrazing\"] = []\n",
    "    i[\"microcatchment\"]= []\n",
    "    i[\"seedbank\"] = []\n",
    "    i[\"seeding\"] = []\n",
    "    i[\"waterpoint\"] = []\n",
    "    i[\"completed\"] = True\n",
    "    i[\"rangeModuleId\"] = 1\n",
    "    i[\"currentStep\"]= \"DONE\"\n",
    "    i[\"moduleId\"] = 6\n",
    "    i[\"is_revisit\"] = False\n",
    "    #get the date_collected\n",
    "    for j in records:\n",
    "        if i[\"key\"] == j[\"KEY\"]:\n",
    "            i[\"date_collected\"] = datetime.strptime(j['basic_info-date'], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "    #match project id\n",
    "    for x in projects_list:\n",
    "        if i['key'] == x['key']:\n",
    "            i['projectId'] = x['project']['id']\n",
    "    #match plot id\n",
    "    for k in plot_list:\n",
    "        if i['key'] == k['key']:\n",
    "            i['plotId'] = k['plot']['id']\n",
    "    #match farming_entity_id\n",
    "    for h in farming_entity_list:\n",
    "        if i['key'] == h['key']:\n",
    "            i['farmingEntityId'] = h['farmingEntity']['id']\n",
    "    #match enumerator id\n",
    "    for e in enums:\n",
    "        if i['key'] == e['key']:\n",
    "            i['enumeratorId'] = e['id']\n",
    "\n",
    "\n",
    "ic(len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "095b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(final, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3600d",
   "metadata": {},
   "source": [
    "## clean up keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1265e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up keys\n",
    "def remove_keys(obj, key=\"key\"):\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(key, None)\n",
    "        for v in obj.values():\n",
    "            remove_keys(v, key)\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            remove_keys(v, key)\n",
    "    return obj\n",
    "\n",
    "finaal = [x for x in final if x['key'] == 'uuid:b4a2d545-66fe-4d23-825b-41025149b460']\n",
    "finaal = [remove_keys(i) for i in final]\n",
    "\n",
    "#add id\n",
    "for idx, c in enumerate(finaal):\n",
    "    c['id'] = idx + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4d406414",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'to_delete.json'), 'w') as f:\n",
    "    json.dump(finaal, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a329cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
